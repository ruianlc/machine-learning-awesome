{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=8>name</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 项目说明"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "，。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据说明"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "，。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建模目的"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "。、"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 依赖库及公共函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 依赖库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1/依赖库\n",
    "\"\"\"\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from warnings import filterwarnings\n",
    "from time import time\n",
    "from random import seed, shuffle, sample, choice\n",
    "\n",
    "# from sklearnex import patch_sklearn\n",
    "# patch_sklearn()\n",
    "\n",
    "from sklearn.metrics import make_scorer, accuracy_score, r2_score, log_loss, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "from sklearn.model_selection import cross_val_score, KFold, GridSearchCV, StratifiedKFold, StratifiedGroupKFold, RepeatedStratifiedKFold, TimeSeriesSplit\n",
    "from sklearn.model_selection import learning_curve, validation_curve\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "sns.set_style(\"ticks\")\n",
    "pd.set_option(\"display.max_rows\", 50)\n",
    "pd.set_option(\"display.max_columns\", 20)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "plt.rcParams[\"figure.max_open_warning\"] = 100\n",
    "plt.rcParams['font.sans-serif']=['Simhei']\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "\"\"\"\n",
    "2/全局变量设置\n",
    "\"\"\"\n",
    "random_state = 42\n",
    "seed(random_state)\n",
    "np.random.seed(random_state)\n",
    "\n",
    "# 交叉验证\n",
    "cv_skf = StratifiedKFold(n_splits=5, shuffle=False, random_state=None)\n",
    "#cv_sgkf = StratifiedGroupKFold(n_splits=3, shuffle=True, random_state=random_state)\n",
    "cv_sgkf = StratifiedGroupKFold(n_splits=3, shuffle=False)\n",
    "\n",
    "# 模型评价\n",
    "scoring_acc = make_scorer(accuracy_score, greater_is_better=True, needs_proba=False)\n",
    "scoring_log = make_scorer(log_loss, greater_is_better=False, needs_proba=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 公共函数定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 建模相关"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "1/数据探索相关函数\n",
    "\"\"\"\n",
    "# 相关系数热力图矩阵\n",
    "def plot_corr(corr, thresh=.0):\n",
    "    kot = corr[abs(corr)>=thresh]\n",
    "    plt.figure(figsize=(17, 17))\n",
    "    #c_map = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "    c_map = 'coolwarm_r'\n",
    "    sns.heatmap(kot, annot=True, fmt=\".2f\", cmap=c_map, square=True, center=0.0)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# 特征数据分布\n",
    "def plot_distribution(data,feature_names,figsize=(4.5, 4)):\n",
    "    for name in feature_names:\n",
    "        plt.figure(figsize=figsize)\n",
    "        sns.histplot(x=name, data=data)\n",
    "        plt.show()\n",
    "\n",
    "def plot_distribution_multi(data,feature_names,figsize=(12,10)):\n",
    "    # Plots the histogram for each numerical feature in a separate subplot\n",
    "    df_X = data[feature_names]\n",
    "    df_X.hist(bins=25, figsize=figsize, layout=(-1, 4), edgecolor=\"black\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def sns_distribution(data, feature_names, hue=None, col=None, figsize=(12,8)):\n",
    "    #plt.rcParams['figure.figsize'] = figsize\n",
    "    fig = plt.figure(figsize = figsize)\n",
    "    #plt.style.use(\"ggplot\")\n",
    "    for name in feature_names:\n",
    "        if hue:\n",
    "            if col:\n",
    "                sns.displot(data, x=name, hue=hue, col=col, kde=True)\n",
    "            else:\n",
    "                sns.displot(data, x=name, hue=hue, kde=True)\n",
    "        else:\n",
    "            sns.displot(data, x=name, kde=True)\n",
    "\n",
    "def cmshow(mdl, X, y, label_names):\n",
    "    y_hat= mdl.predict(X)\n",
    "    test_score = evaluate_prediction(y, y_hat, accuracy_score, \"test\")\n",
    "    print(f\"准确率：{test_score}\")\n",
    "\n",
    "    conf_mat = confusion_matrix(y_true=y, y_pred=y_hat)\n",
    "\n",
    "    cm_obj = ConfusionMatrixDisplay(conf_mat, display_labels=label_names)\n",
    "    cm_obj.plot()\n",
    "    plt.xlabel(\"预测结果\")\n",
    "    plt.ylabel(\"实际结果\")\n",
    "    plt.title(\"混淆矩阵：实际值 vs 预测值\")\n",
    "    #plt.savefig(\"Confusion_Matrix.png\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "def checkmissing(df):\n",
    "    missing = (df.isna().sum()/df.shape[0]*100).to_frame().reset_index().rename(columns = {'index':'column', 0:'pct_missing'})\n",
    "    fig, ax = plt.subplots()\n",
    "    ax = sns.barplot(data = missing, x = 'column', y = 'pct_missing')\n",
    "    ax.set_ylabel('% Missing Values')\n",
    "    ax.set_xlabel('Column Names')\n",
    "    plt.xticks(rotation = 90)\n",
    "    plt.show()\n",
    "\n",
    "\"\"\"\n",
    "2/建模相关函数\n",
    "\"\"\"\n",
    "# 分类模型预测准确率\n",
    "def evaluate_prediction(y, y_hat, score_func, set_name=\"test\"):\n",
    "    score = score_func(y, y_hat)\n",
    "    print(f\"\\n{set_name} score: {score}\\n\")\n",
    "    print(\"\\nconfusion_matrix:\\n\")\n",
    "    print(confusion_matrix(y, y_hat))\n",
    "    print(\"\\nclassification_report:\\n\")\n",
    "    print(classification_report(y, y_hat, digits=6))\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "# %% 学习曲线\n",
    "def plot_learning_curve(\n",
    "    estimator,\n",
    "    title,\n",
    "    X,\n",
    "    y,\n",
    "    axes=None,\n",
    "    ylim=None,\n",
    "    cv=None,\n",
    "    n_jobs=None,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    groups=None,\n",
    "):\n",
    "    if axes is None:\n",
    "        _, axes = plt.subplots(1, 3, figsize=(20, 5))\n",
    "\n",
    "    axes[0].set_title(title)\n",
    "    if ylim is not None:\n",
    "        axes[0].set_ylim(*ylim)\n",
    "    axes[0].set_xlabel(\"Training examples\")\n",
    "    axes[0].set_ylabel(\"Score\")\n",
    "\n",
    "    train_sizes, train_scores, test_scores, fit_times, _ = learning_curve(\n",
    "        estimator,\n",
    "        X,\n",
    "        y,\n",
    "        cv=cv,\n",
    "        n_jobs=n_jobs,\n",
    "        train_sizes=train_sizes,\n",
    "        return_times=True,\n",
    "        groups=groups,\n",
    "    )\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "    fit_times_mean = np.mean(fit_times, axis=1)\n",
    "    fit_times_std = np.std(fit_times, axis=1)\n",
    "\n",
    "    # Plot learning curve\n",
    "    axes[0].grid()\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        train_scores_mean - train_scores_std,\n",
    "        train_scores_mean + train_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"r\",\n",
    "    )\n",
    "    axes[0].fill_between(\n",
    "        train_sizes,\n",
    "        test_scores_mean - test_scores_std,\n",
    "        test_scores_mean + test_scores_std,\n",
    "        alpha=0.1,\n",
    "        color=\"g\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, train_scores_mean, \"o-\", color=\"r\", label=\"Training score\"\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        train_sizes, test_scores_mean, \"o-\", color=\"g\", label=\"Cross-validation score\"\n",
    "    )\n",
    "    axes[0].legend(loc=\"best\")\n",
    "\n",
    "    # Plot n_samples vs fit_times\n",
    "    axes[1].grid()\n",
    "    axes[1].plot(train_sizes, fit_times_mean, \"o-\")\n",
    "    axes[1].fill_between(\n",
    "        train_sizes,\n",
    "        fit_times_mean - fit_times_std,\n",
    "        fit_times_mean + fit_times_std,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Training examples\")\n",
    "    axes[1].set_ylabel(\"fit_times\")\n",
    "    axes[1].set_title(\"Scalability of the model\")\n",
    "\n",
    "    # Plot fit_time vs score\n",
    "    fit_time_argsort = fit_times_mean.argsort()\n",
    "    fit_time_sorted = fit_times_mean[fit_time_argsort]\n",
    "    test_scores_mean_sorted = test_scores_mean[fit_time_argsort]\n",
    "    test_scores_std_sorted = test_scores_std[fit_time_argsort]\n",
    "    axes[2].grid()\n",
    "    axes[2].plot(fit_time_sorted, test_scores_mean_sorted, \"o-\")\n",
    "    axes[2].fill_between(\n",
    "        fit_time_sorted,\n",
    "        test_scores_mean_sorted - test_scores_std_sorted,\n",
    "        test_scores_mean_sorted + test_scores_std_sorted,\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    axes[2].set_xlabel(\"fit_times\")\n",
    "    axes[2].set_ylabel(\"Score\")\n",
    "    axes[2].set_title(\"Performance of the model\")\n",
    "\n",
    "    return plt\n",
    "\n",
    "# 交叉验证，超参优化\n",
    "class SuperEstimator:\n",
    "    def __init__(self, searchcv, multiple=0):\n",
    "        self.estimator = searchcv.estimator\n",
    "        self.searchcv = searchcv\n",
    "        self.multiple = multiple\n",
    "\n",
    "    def _best_mean_std(self, cv_results_):\n",
    "        \"\"\"\n",
    "        基于验证集成绩的均值与标准差，算出一个综合的成绩。\n",
    "        \"\"\"\n",
    "        scores = np.array(cv_results_[\"mean_test_score\"]) - self.multiple * np.array(\n",
    "            cv_results_[\"std_test_score\"]\n",
    "        )\n",
    "        best_idx = np.argmax(scores)\n",
    "        return best_idx\n",
    "\n",
    "    def search(self, X, y=None, groups=None, verbose=True, n_jobs=-1):\n",
    "        \"\"\"\n",
    "        超参优化。\n",
    "        \"\"\"\n",
    "        t0 = time()\n",
    "        self.searchcv.fit(X, y=y, groups=groups)\n",
    "        t1 = time()\n",
    "        best_idx = self._best_mean_std(self.searchcv.cv_results_)\n",
    "        best_params = dict(self.searchcv.cv_results_[\"params\"][best_idx])\n",
    "        self.estimator.set_params(**best_params)\n",
    "        scores_cv = cross_val_score(\n",
    "            self.estimator,\n",
    "            X,\n",
    "            y=y,\n",
    "            groups=groups,\n",
    "            scoring=self.searchcv.scoring,\n",
    "            cv=self.searchcv.cv,\n",
    "            n_jobs=n_jobs,\n",
    "        )\n",
    "        mu = np.mean(scores_cv)\n",
    "        #best_score = np.max(scores_cv)\n",
    "        if verbose:\n",
    "            print(f\"\\nSearching elapses {t1 - t0:.6f} seconds.\\n\")\n",
    "            print(f\"\\nbest_params: {best_params}\\n\")\n",
    "            print(f\"\\ncv scores：{scores_cv}\\n\")\n",
    "            sigma = np.std(scores_cv)\n",
    "            bounds_left = mu - 1.96 * sigma\n",
    "            bounds_right = mu + 1.96 * sigma\n",
    "            print(\n",
    "                f\"\\ncv scores(95% CI): {mu:.6f} ± {1.96 * sigma:.6f} = \"\n",
    "                f\"({bounds_left:.6f}, {bounds_right:.6f})\\n\"\n",
    "            )\n",
    "        self.estimator.fit(X, y)\n",
    "        return best_params, mu, sigma\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据文件处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "\n",
    "def rand_resampling(X, y, class_range, choose_times):\n",
    "    \"\"\"\n",
    "    :describe: 对分类模型原始样本进行随机重采样，提高样本数据量\n",
    "    :author: anrui\n",
    "    :date: 2022.04.29\n",
    "    :modified: 2022.11.03\n",
    "    :param X: 特征数据集\n",
    "    :param y: 标签数据集\n",
    "    :param class_range: 类别范围\n",
    "    :param choose_times: 每种类别循环随机取的次数\n",
    "    :return: X_new, y_new\n",
    "    \"\"\"\n",
    "    X_new = []\n",
    "    y_new = []\n",
    "    for cc in class_range:\n",
    "        X_sub = X[y == cc, :] # 一个类别所有特征数据\n",
    "        y_sub = y[y == cc]    # 一个类别所有类别数据\n",
    "\n",
    "        x_sub_tmp = X_sub.copy()         # 一个类别重采样后的样本\n",
    "        y_sub_tmp = y_sub.copy()\n",
    "        for i in range(0, choose_times):  # 多次随机采样\n",
    "            seed(random_state + i)  # 初始种子一致，保证不同循序次数重采样结果不同\n",
    "            for j in range(X_sub.shape[0] - 3):\n",
    "                # 单个样本与其后的任意随机两个样本计算均值，得到新样本\n",
    "                grp_X = X_sub[j, :].reshape(1, -1)  # 一个类别的单个样本\n",
    "                the_last = X_sub[j + 1:-1, :]\n",
    "                row_idx = np.arange(the_last.shape[0])\n",
    "                shuffle(row_idx)\n",
    "                choose_sample = the_last[row_idx[0:2], :]\n",
    "                new_sample_tmp = np.append(grp_X, choose_sample, axis=0)  # 按行拼接\n",
    "                new_sample = np.average(new_sample_tmp, axis=0)  # 按列求均值\n",
    "                x_sub_tmp = np.r_[x_sub_tmp, new_sample.reshape(1, -1)]\n",
    "                y_sub_tmp = np.r_[y_sub_tmp, cc]\n",
    "\n",
    "        X_new += x_sub_tmp.tolist()\n",
    "        y_new += y_sub_tmp.tolist()\n",
    "        \n",
    "    return np.array(X_new), np.array(y_new)\n",
    "\n",
    "\n",
    "def rand_resampling_stratify_group(X, y, groups_train, choose_times):\n",
    "    \"\"\"\n",
    "    :describe: 对分类模型原始样本进行随机重采样，提高样本数据量。分层分组重采样（本质上也为分层抽样）\n",
    "    :author: anrui\n",
    "    :date: 2022.10.15\n",
    "    :param X: 特征数据集\n",
    "    :param y: 标签数据集\n",
    "    :param groups_train: 组号标签\n",
    "    :param choose_times: 每种类别循环随机取的次数\n",
    "    :return: X_new, y_new, group_new\n",
    "    \"\"\"\n",
    "    X_new = []\n",
    "    y_new = []\n",
    "    group_new = []\n",
    "\n",
    "    groups_list = np.unique(groups_train).tolist()\n",
    "\n",
    "    for gg in groups_list:\n",
    "        # 单个组进行抽样\n",
    "        X_sub = X[groups_train == gg, :]             # 一个组号所有特征数据\n",
    "        y_sub = y[groups_train == gg]                # 一个组号所有类别数据\n",
    "        group_sub = groups_train[groups_train == gg] # 一个组号所有组号数据\n",
    "\n",
    "\n",
    "        x_sub_tmp = X_sub.copy()         # 一个组号重采样后的样本\n",
    "        y_sub_tmp = y_sub.copy()\n",
    "        group_sub_tmp = group_sub.tolist().copy()\n",
    "\n",
    "        for i in range(0, choose_times):  # 多次随机采样\n",
    "            seed(random_state + i)  # 初始种子一致，保证不同循序次数重采样结果不同\n",
    "            for j in range(X_sub.shape[0] - 3):\n",
    "                # 单个样本与其后的任意随机两个样本计算均值，得到新样本\n",
    "                grp_X = X_sub[j, :].reshape(1, -1)  # 一个类别的单个样本\n",
    "                the_last = X_sub[j + 1:-1, :]\n",
    "                row_idx = np.arange(the_last.shape[0])\n",
    "                shuffle(row_idx)\n",
    "                choose_sample = the_last[row_idx[0:2], :]\n",
    "                new_sample_tmp = np.append(grp_X, choose_sample, axis=0)  # 按行拼接\n",
    "                new_sample = np.average(new_sample_tmp, axis=0)  # 按列求均值\n",
    "                x_sub_tmp = np.r_[x_sub_tmp, new_sample.reshape(1, -1)]\n",
    "                y_sub_tmp = np.r_[y_sub_tmp, int(gg[:2])] # 组号为string类型，前两个字符为类别编码\n",
    "                group_sub_tmp.append(gg)\n",
    "\n",
    "        X_new += x_sub_tmp.tolist()\n",
    "        y_new += y_sub_tmp.tolist()\n",
    "        group_new += group_sub_tmp\n",
    "        \n",
    "    return np.array(X_new), np.array(y_new), np.asarray(group_new)\n",
    "    \n",
    "def grouped_avg(myArray, N=2):\n",
    "    \"\"\"\n",
    "    :describe: 数组多行平均\n",
    "    :author: anrui\n",
    "    :date: 2022.04.20\n",
    "    :param myArray: 原始二维数据\n",
    "    :param N: 按行平均的行数\n",
    "    :return: result\n",
    "    \"\"\"\n",
    "    result = np.cumsum(myArray, 0)[N-1::N]/float(N)\n",
    "    result[1:] = result[1:] - result[:-1]\n",
    "    return result\n",
    "\n",
    "def get_project_rootpath():\n",
    "    \"\"\"\n",
    "    :describe: 获取项目根目录\n",
    "    :author: anrui\n",
    "    :date: 2022.05.19\n",
    "    :param \n",
    "    :return: result\n",
    "    \"\"\"\n",
    "    path = os.path.realpath(os.curdir)\n",
    "    while (True):\n",
    "        for subpath in os.listdir(path):\n",
    "            if '.idea' in subpath or '.vscode' in subpath:  # 根目录下pycharm项目必然存在'.idea'文件，同理vscode项目必然存在'.vscode'文件\n",
    "                return path\n",
    "        path = os.path.dirname(path)\n",
    "\n",
    "def mk_dir(path):\n",
    "    # 去除首位空格\n",
    "    path = path.strip()\n",
    "    # 去除尾部 \\ 符号\n",
    "    path = path.rstrip(\"\\\\\")\n",
    "    # 判断路径是否存在\n",
    "    # 存在    返回 True\n",
    "    # 不存在  返回 False\n",
    "    isExists = os.path.exists(path)\n",
    "    # 判断结果\n",
    "    if not isExists:\n",
    "        # 如果不存在则创建目录\n",
    "        # 创建目录操作函数\n",
    "        os.makedirs(path)\n",
    "        return True\n",
    "    else:\n",
    "        # 如果目录存在则不创建，并提示目录已存在\n",
    "        return False\n",
    "\n",
    "def copy_dir(src_dir, des_dir):\n",
    "    \"\"\"\n",
    "    :describe: 文件目录拷贝\n",
    "    :author: anrui\n",
    "    :date: 2022.09.24\n",
    "    :param src_dir: 原始目录\n",
    "    :param des_dir: 目标目录\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    if os.path.exists(des_dir):\n",
    "        print(\"目标文件目录已存在，先删除\")\n",
    "        shutil.rmtree(des_dir)\n",
    "    print(\"拷贝文件开始...\")\n",
    "    shutil.copytree(src_dir, des_dir)\n",
    "    print(\"拷贝文件结束...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".、"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# name建模"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对建模目标进行编码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将建模target进行编码\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "df = df[f_names + [target]]\n",
    "\n",
    "# 编码之前保存原始类别名称\n",
    "level_names_tmp = np.unique(df[target])\n",
    "\n",
    "le = LabelEncoder()\n",
    "df[target] = le.fit_transform(df[[target]])\n",
    "level_codes_tmp = np.unique(df[target])\n",
    "\n",
    "# 建立等级名称与等级编码的字典\n",
    "label_decoder = dict(zip(level_codes_tmp, level_names_tmp))\n",
    "label_encoder = {val: key for key, val in label_decoder.items()}\n",
    "print(label_decoder)\n",
    "print(label_encoder)\n",
    "\n",
    "# 所有等级名称\n",
    "label_names = level_names_tmp.tolist()\n",
    "class_range = range(0, len(label_decoder))\n",
    "\n",
    "label_names_old = list(label_names).copy()\n",
    "class_range_old = list(class_range).copy()\n",
    "label_decoder_old = label_decoder.copy()\n",
    "\n",
    "class_nums = len(class_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.save(r'data/output/label_decoder.npy', label_decoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据集划分"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 对训练集进行重采样"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 数据探索"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义建模的自变量与因变量\n",
    "建模所用的自变量为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names_X = f_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "样本数量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集样本大小：\n",
      "(231920, 226)\n",
      "(231920,)\n",
      "测试集样本大小：\n",
      "(77303, 226)\n",
      "(77303,)\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train[names_X].to_numpy()\n",
    "y_train = df_train[target].to_numpy()\n",
    "\n",
    "groups_train = df_train[\"组号\"].to_numpy()\n",
    "\n",
    "X_test = df_test[names_X].to_numpy()\n",
    "y_test = df_test[target].to_numpy()\n",
    "\n",
    "print(\"训练集样本大小：\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(\"测试集样本大小：\")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型训练与表现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型得分汇总"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from tempfile import mkdtemp\n",
    "from shutil import rmtree\n",
    "\n",
    "# 所有选用的模型\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis as QDA\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import PowerTransformer, QuantileTransformer\n",
    "\n",
    "\n",
    "# 模型得分汇总\n",
    "mdl_names = []\n",
    "mdl_ct_types = []\n",
    "base_estimators = []\n",
    "best_params = []\n",
    "cv_scores = []\n",
    "cv_scores_std = []\n",
    "mdl_times = []\n",
    "\n",
    "\n",
    "def model_train(X_train, y_train, groups_train=None, method=None, ct_type=None):\n",
    "    name = method\n",
    "    \n",
    "    t0 = time()\n",
    "    if method == \"Logistic Regression\":\n",
    "        classifier = LogisticRegression(random_state=random_state, max_iter=5000)\n",
    "        param_grid = {\n",
    "            'model__C': [0.01, 0.1, 1, 10, 100],\n",
    "        }\n",
    "    elif method == \"QDA\":\n",
    "        classifier = QDA()\n",
    "        param_grid = {\n",
    "            'model__reg_param': np.logspace(-12, 1, 20),\n",
    "            # 'spcprep__spectrum__kbest__k': range(4, 12),\n",
    "            # 'spcprep__SNV_spectrum__kbest__k': range(4, 12),\n",
    "        }\n",
    "    elif method == \"GaussianNB\":\n",
    "        classifier = GaussianNB()\n",
    "        param_grid = {\n",
    "            'model__var_smoothing': np.logspace(-10, 3, 20),\n",
    "        }\n",
    "    elif method == \"SVMClassifier\":\n",
    "        classifier = SVC(probability=True) # with score_log\n",
    "        param_grid = {\n",
    "            #'model__kernel':['poly', 'rbf'],\n",
    "            'model__C': [0.1, 1, 10, 100, 500],\n",
    "            'model__gamma': [100, 10, 1, 0.1, 0.01],\n",
    "        }\n",
    "    elif method == \"KNN\":\n",
    "        classifier = KNeighborsClassifier()\n",
    "        param_grid = {\n",
    "            'model__n_neighbors': np.arange(2,10),\n",
    "        }\n",
    "    elif method == \"Decision Tree\":\n",
    "        classifier = DecisionTreeClassifier(\n",
    "            random_state=random_state\n",
    "        )\n",
    "        param_grid = {\n",
    "            'model__max_depth': [2, 5, 10, 20],\n",
    "            'model__min_samples_leaf': [5, 10, 20, 50, 100],\n",
    "            'model__criterion': [\"gini\", \"entropy\"]\n",
    "        }\n",
    "    elif method == \"Random Forest\":\n",
    "        classifier = RandomForestClassifier(\n",
    "            random_state=random_state,\n",
    "        )\n",
    "        param_grid = {\n",
    "            'model__max_depth': range(3, 10),\n",
    "            'model__min_samples_leaf': range(1, 6),\n",
    "            'model__min_samples_split': range(2, 10),\n",
    "        }\n",
    "    elif method == \"XGBoost\":\n",
    "        classifier = XGBClassifier(\n",
    "            eval_metric='mlogloss',\n",
    "            use_label_encoder=False,\n",
    "        )\n",
    "        param_grid = {\n",
    "            'model__learning_rate': np.arange(0.01, 0.1, 0.01),\n",
    "            'model__min_child_weight': [1, 5, 10],\n",
    "            'model__gamma': [0.5, 1, 1.5, 2, 5],\n",
    "            'model__subsample': [0.6, 0.8, 1.0],\n",
    "            'model__colsample_bytree': [0.6, 0.8, 1.0],\n",
    "            'model__max_depth': range(2, 10, 1),\n",
    "        }\n",
    "    elif method == \"LightGBM\":\n",
    "        classifier = LGBMClassifier(\n",
    "            boosting_type='gbdt',\n",
    "            objective='multiclass',\n",
    "            metric='multi_logloss',\n",
    "            num_iterations=1000,\n",
    "        )\n",
    "        param_grid = {\n",
    "            'model__learning_rate': np.arange(0.002, 0.02, 0.005),\n",
    "            'model__num_leaves': range(30, 80, 10),\n",
    "            #'model__reg_alpha': [0.1, 0.5],\n",
    "            #'model__min_data_in_leaf': [30, 50, 100, 300, 400],\n",
    "            #'model__lambda_l1': [0, 1, 1.5],\n",
    "            #'model__lambda_l2': [0, 1],\n",
    "        }\n",
    "    else:\n",
    "        print(\"no such method...\")\n",
    "        return\n",
    "\n",
    "    spectrum_idxs = list(range(X_train.shape[1]))[:len(names_X):]\n",
    "    cachedir = mkdtemp() # 创建缓存文件\n",
    "        \n",
    "    if ct_type == \"SPC_Remain\":\n",
    "        ct = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\n",
    "                    'SPC_Remain',\n",
    "                    Pipeline(\n",
    "                        steps=[\n",
    "                            ('spc_remain', SPC_Remain()),\n",
    "                            ('scale', StandardScaler()),\n",
    "                            ('pca', PCA(n_components=0.9995,random_state=random_state)),\n",
    "                        ]),\n",
    "                    spectrum_idxs,\n",
    "                ),\n",
    "            ],\n",
    "            remainder='passthrough',\n",
    "        )\n",
    "    elif ct_type == \"CWT\":\n",
    "        ct = ColumnTransformer(\n",
    "            transformers=[\n",
    "                (\n",
    "                    'CWT',\n",
    "                    Pipeline(\n",
    "                        steps=[\n",
    "                            ('cwt', CWT(range(9, 10))),\n",
    "                            ('scale', StandardScaler()),\n",
    "                            ('pca', PCA(n_components=0.9995,random_state=random_state)),\n",
    "                        ]),\n",
    "                    spectrum_idxs,\n",
    "                ),\n",
    "            ],\n",
    "            remainder='passthrough',\n",
    "        )\n",
    "    else:\n",
    "        print(\"wrong ct_type...\")\n",
    "        return\n",
    "\n",
    "    pipeline = Pipeline(\n",
    "        steps=[\n",
    "            ('spcprep', ct),\n",
    "            ('model', classifier),\n",
    "        ],\n",
    "        memory=cachedir,\n",
    "    )\n",
    "\n",
    "    # 时间序列交叉验证\n",
    "    custom_splitter = StratifiedGroupTimeSeriesSplit(X_train, y_train, groups_train, n_splits=2)\n",
    "\n",
    "    searchcv = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=custom_splitter,\n",
    "        scoring=scoring_log,\n",
    "        verbose=0,\n",
    "        refit=False,\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "\n",
    "    superestimator = SuperEstimator(searchcv)\n",
    "    best_params, cv_score, cv_score_std = superestimator.search(X_train, y_train.astype('int'), groups=groups_train)\n",
    "    mdl = superestimator.estimator\n",
    "\n",
    "    ctime = round(time() - t0, 3)\n",
    "    rmtree(cachedir, ignore_errors=True) # 删除缓存文件\n",
    "\n",
    "    return ctime, name, ct_type, cv_score, cv_score_std, best_params, mdl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型训练及成绩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [\"Random Forest\", \"SVMClassifier\"]#, \"Logistic Regression\", \"Decision Tree\", \"GaussianNB\", \"KNN\", \"QDA\", \"KNN\", \"LightGBM\", \"XGBoost\"]\n",
    "ct_types = [\"CWT\"]\n",
    "\n",
    "for model in model_list:\n",
    "    for ct_type in ct_types:\n",
    "        print(f\"++++++++++++++++++++++++训练模型：{model}--{ct_type}++++++++++++++++++++++++\\n\")\n",
    "        ctime, name, ct_type, cv_score, cv_score_std, best_param, mdl = model_train(X_train, y_train, groups_train, method=model, ct_type=ct_type)\n",
    "\n",
    "        mdl_names.append(name)\n",
    "        mdl_ct_types.append(ct_type)\n",
    "        base_estimators.append((name, ct_type, mdl))\n",
    "        best_params.append((name, best_param))\n",
    "        cv_scores.append(cv_score)\n",
    "        cv_scores_std.append(cv_score_std)\n",
    "        mdl_times.append(ctime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdls = pd.DataFrame(\n",
    "    data=np.hstack((\n",
    "        np.array(mdl_names).reshape(-1,1),\n",
    "        np.array(mdl_ct_types).reshape(-1,1),\n",
    "        np.array(cv_scores).reshape(-1,1),\n",
    "        np.array(cv_scores_std).reshape(-1,1),\n",
    "        np.array(mdl_times).reshape(-1,1),\n",
    "    )), \n",
    "    columns=[\"mdl_names\", \"mdl_ct_types\", \"cv_score\", \"cv_scores_std\", \"model_time (s)\"]\n",
    ")\n",
    "print(\"++++++++++++++++++++++++所有模型成绩++++++++++++++++++++++++\")\n",
    "mdls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型选择"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "\n",
    "\n",
    "set_config(display=\"diagram\")\n",
    "mdl = base_estimators[np.argmax(cv_scores)][2]\n",
    "#mdl = base_estimators[2][2]\n",
    "print(\"++++++++++++++++++++++++最佳模型++++++++++++++++++++++++\")\n",
    "mdl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型持久化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型\n",
    "import joblib\n",
    "\n",
    "mdl_path = r'data/output/model_.pkl'\n",
    "joblib.dump(mdl, mdl_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "展示transformer之后得到的特征名称："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mdl.named_steps['spcprep'].named_transformers_['PCA_MSC'].named_steps['PCA'].get_feature_names_out().tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试集的表现"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用全部测试集进行模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmshow(mdl, X_test, y_test, label_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用保留下来的外部验证集验证模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_val.info()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 外测集的表现"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用全部外测集进行模型验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载和使用模型\n",
    "mdl = joblib.load(r'data/output/model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, y_val= df_val[names_X].to_numpy(), df_val[target].to_numpy()\n",
    "\n",
    "cmshow(mdl, X_val, y_val, label_names)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 结论"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ./\n",
    "* ,."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e42634819b8c191a5d07eaf23810ff32516dd8d3875f28ec3e488928fbd3c187"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
